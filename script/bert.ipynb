{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512 #75\n",
    "bs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_vals = list([\"O\", \"I\", \"B\"])\n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213450/213450 [00:01<00:00, 156029.80B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus(filename):\n",
    "    corpus = []\n",
    "    soup = BeautifulSoup(open(filename, \"r\"), \"lxml\")\n",
    "    idx = 0\n",
    "    full_labels = []\n",
    "    for r in soup.find_all('review'):\n",
    "#         if idx>=1:\n",
    "#             break\n",
    "        doc = \"\"\n",
    "        labels = []\n",
    "        num_chars=0\n",
    "        for idx, sentence in enumerate(r.find_all('sentence')):\n",
    "            sent = sentence.text.strip()\n",
    "#             print(sent)\n",
    "            try:\n",
    "                from_num = int(sentence.opinion.get(\"from\"))\n",
    "                to_num = int(sentence.opinion.get(\"to\"))\n",
    "            except:\n",
    "                from_num=0\n",
    "                to_num=0\n",
    "            labels.append([num_chars+from_num+1,num_chars+to_num+1])\n",
    "            doc += \" \" + sent\n",
    "            num_chars = num_chars + (len(sent))+1\n",
    "        k=0\n",
    "        j=0\n",
    "        for i in range(len(doc)):\n",
    "            if k>=len(labels):\n",
    "                break\n",
    "            l = labels[k]\n",
    "            if doc[i]==\" \":\n",
    "                if i==l[1]:\n",
    "                    labels[k][1]=labels[k][1]-j\n",
    "#                     print(i, doc[:i+1], \"   \",l,\"  \",j)\n",
    "                    k+=1   \n",
    "                j+=1\n",
    "            if i==l[0]:\n",
    "                labels[k][0]=labels[k][0]-j\n",
    "#                 print(i, doc[:i+1], \"   \",l,\"  \",j)\n",
    "            if doc[i]!=\" \" and i==l[1]:\n",
    "                labels[k][1]=labels[k][1]-j\n",
    "#                 print(i, doc[:i+1], \"   \",l,\"  \",j)\n",
    "                k+=1\n",
    "                \n",
    "#             if l[0]!=l[1]:\n",
    "#                 for i in range(len(sent)):\n",
    "#                     if sent[i]==\" \":\n",
    "#                         j+=1\n",
    "#                     if i==from_num:\n",
    "#                         from_num=from_num-j\n",
    "#                     if i==to_num:\n",
    "#                         to_num=to_num-j\n",
    "        corpus.append(doc)\n",
    "        full_labels.append(labels)\n",
    "    return corpus, full_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, char_labels = generate_corpus(\"../data/official_data/EN_REST_SB1_TEST.xml.gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\" Yum! Serves really good sushi. Not the biggest portions but adequate. Green Tea creme brulee is a must! Don't leave the restaurant without it.\",\n",
       " [[0, 0], [20, 25], [39, 47], [59, 78], [86, 86]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0],char_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y',\n",
       " '##um',\n",
       " '!',\n",
       " 'serves',\n",
       " 'really',\n",
       " 'good',\n",
       " 'su',\n",
       " '##shi',\n",
       " '.',\n",
       " 'not',\n",
       " 'the',\n",
       " 'biggest',\n",
       " 'portions',\n",
       " 'but',\n",
       " 'adequate',\n",
       " '.',\n",
       " 'green',\n",
       " 'tea',\n",
       " 'c',\n",
       " '##rem',\n",
       " '##e',\n",
       " 'br',\n",
       " '##ule',\n",
       " '##e',\n",
       " 'is',\n",
       " 'a',\n",
       " 'must',\n",
       " '!',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'leave',\n",
       " 'the',\n",
       " 'restaurant',\n",
       " 'without',\n",
       " 'it',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in corpus]\n",
    "tokenized_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['y', '##um', '!', 'serves', 'really', 'good', 'su', '##shi', '.', 'not', 'the', 'biggest', 'portions', 'but', 'adequate', '.', 'green', 'tea', 'c', '##rem', '##e', 'br', '##ule', '##e', 'is', 'a', 'must', '!', 'don', \"'\", 't', 'leave', 'the', 'restaurant', 'without', 'it', '.']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "['no', 'comparison', '–', 'i', 'can', \"'\", 't', 'say', 'enough', 'about', 'this', 'place', '.', 'it', 'has', 'great', 'su', '##shi', 'and', 'even', 'better', 'service', '.', 'the', 'entire', 'staff', 'was', 'extremely', 'a', '##cco', '##mo', '##dating', 'and', 'tended', 'to', 'my', 'every', 'need', '.', 'i', \"'\", 've', 'been', 'to', 'this', 'restaurant', 'over', 'a', 'dozen', 'times', 'with', 'no', 'complaints', 'to', 'date', '.']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for idx, text in enumerate(tokenized_texts):\n",
    "    curr_labels = []\n",
    "    k=0\n",
    "    i=0\n",
    "    for curr_idx, word in enumerate(text):           \n",
    "        for char in word:\n",
    "            if char!=\"#\":\n",
    "                if k>=len(char_labels[idx]):\n",
    "                    break\n",
    "                if i>char_labels[idx][k][1]:\n",
    "                    k+=1\n",
    "                    if k>=len(char_labels[idx]):\n",
    "                        break\n",
    "                if char_labels[idx][k][0]!=char_labels[idx][k][1]:\n",
    "                    if i == char_labels[idx][k][0]:\n",
    "                        curr_labels.append(\"B\")\n",
    "                    elif (char_labels[idx][k][0]<i and i<=char_labels[idx][k][1]-1):\n",
    "                        if len(curr_labels)!=curr_idx+1:\n",
    "                            curr_labels.append(\"I\")\n",
    "                i+=1\n",
    "        if len(curr_labels)!=curr_idx+1:\n",
    "            curr_labels.append(\"O\")\n",
    "    if idx<2:\n",
    "        print(text)\n",
    "        print(curr_labels)\n",
    "        print()\n",
    "    labels.append(curr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pad_sequence([torch.tensor(tokenizer.convert_tokens_to_ids(txt)) for txt in tokenized_texts],batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([90, 497])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pad_sequence([torch.tensor([tag2idx.get(l) for l in lab]) for lab in labels],batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([90, 497])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids.numpy(), tags.numpy(), random_state=2018, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids.numpy(), random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf3fac7983b4c2fba5b5d72c274eb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cfada48c0f40dab01eefe153936dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward. . \n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# for _ in trange(epochs, desc=\"Epoch\"):\n",
    "for _ in tqdm_notebook(range(epochs)):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in tqdm_notebook(enumerate(train_dataloader)):\n",
    "        # add batch to gpu\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # forward pass\n",
    "        print(\"forward. . \")\n",
    "        loss = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_mask, labels=b_labels)\n",
    "        # backward pass\n",
    "        print(loss)\n",
    "        print(\"BPing. . \")\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    # VALIDATION on validation set\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                                  attention_mask=b_input_mask, labels=b_labels)\n",
    "            logits = model(b_input_ids, token_type_ids=None,\n",
    "                           attention_mask=b_input_mask)\n",
    "#         logits = logits.detach().cpu().numpy()\n",
    "        logits = logits.numpy()\n",
    "#         label_ids = b_labels.to('cpu').numpy()\n",
    "        label_ids = b_labels.numpy()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        \n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss/nb_eval_steps\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
    "    valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "    print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0b3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
